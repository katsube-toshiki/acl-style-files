% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% custom packages
\usepackage{booktabs}


\title{Scalable and High-Quality Dataset Construction \\ for Non-English Languages}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Toshiki Katsube \and Taiga Fukuhara \\
%   The University of Tokyo \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Toshiki Katsube\textsuperscript{1}},
 \textbf{Taiga Fukuhara\textsuperscript{1}},
 \textbf{Kenichiro Ando\textsuperscript{2,1}},
 \textbf{Yusuke Mukuta\textsuperscript{1,2}},
\\
 \textbf{Kohei Uehara\textsuperscript{1}},
 \textbf{Tatsuya Harada\textsuperscript{1,2}},
\\
\\
 \textsuperscript{1}The University of Tokyo,
 \textsuperscript{2}RIKEN,
\\
\texttt{katsube@mi.t.u-tokyo.ac.jp}
}

\begin{document}
\maketitle
\begin{abstract}
  This research addresses the pressing issue of the lack of high-quality, large-scale datasets, a critical challenge in the advancement of Japanese Vision and Language (V\&L) models. Existing Japanese V\&L datasets face limitations: those created through manual annotation are difficult to scale, those translated from English datasets lack Japan-specific cultural context, and those automatically collected from the web suffer from quality issues. To overcome these challenges, this paper proposes a novel three-stage methodology for scalably constructing a high-quality V\&L dataset that reflects Japan-specific knowledge and culture. The proposed method consists of (1) collecting images and alt-texts from the web, (2) applying object detection to the collected data, and (3) refining the alt-texts using a Large Language Model (LLM). Using this method, we constructed an image caption dataset of 4 million pairs and a Visual Question Answering (VQA) dataset of 4 million pairs. Results from training a V\&L model on our constructed dataset confirmed a significant improvement in Japanese understanding capabilities compared to training on existing Japanese datasets alone. This research demonstrates the effectiveness of the proposed methodology and contributes to the future progress of Japanese V\&L research.
\end{abstract}

\section{Introduction}

In recent years, research and development of Vision and Language (V\&L) models, which integratively process visual and linguistic information, have rapidly advanced, with expectations for a wide range of applications such as image caption generation, Visual Question Answering (VQA), and image retrieval. V\&L models Typically work like the one shown in Figure \ref{fig:vl_model}.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{images/vlm.pdf}
  \caption{How V\&L Models Work}
  \label{fig:vl_model}
\end{figure*}

Large-scale, high-quality training datasets are indispensable for improving the performance of these V\&L models. However, in V\&L research targeting the Japanese language, the lack of such datasets has become a serious bottleneck.

Existing Japanese V\&L datasets have several issues. Firstly, datasets meticulously annotated by humans (e.g., STAIR Captions \cite{Yoshikawa2017}) are of high quality but are limited in terms of data scale due to the enormous cost and time required for their creation. Secondly, while there are attempts to create Japanese datasets by machine-translating large-scale datasets developed in English-speaking countries (e.g., COCO \cite{lin2014microsoft}), it has been pointed out that Japan-specific contexts, knowledge, or cultural nuances are often lost or unnatural expressions arise during the translation process \cite{sasagawa2024constructing}. Furthermore, approaches that automatically collect images and corresponding alt-texts from the web (e.g., Japanese Image Text Pairs \cite{sasagawa2024constructing}) are effective for large-scale data collection, but they often have significant quality issues, such as texts being inappropriate as natural Japanese or the semantic content of images and texts not matching.

Against this backdrop, the objective of this research is to propose a method for constructing a high-quality and scalable Japanese V\&L dataset that appropriately reflects Japan-specific knowledge and culture. The proposed method consists of a three-stage pipeline: first, applying object detection technology to images and alt-texts collected from the web to assist in understanding image content, and then using a Large Language Model (LLM) to refine the alt-texts into more natural Japanese that is consistent with the image content.

The main contributions of this research are the following three points:

Provision of a large-scale, high-quality Japanese V\&L dataset: Using the proposed method, we constructed an image caption dataset with 4 million pairs and a VQA dataset with 4 million pairs.
Demonstration of improved performance of Japanese V\&L models using the proposed dataset: We experimentally showed that training V\&L models on our constructed dataset improves performance in various V\&L tasks compared to training on existing Japanese datasets.
Establishment of a scalable dataset construction method: By combining automatic collection from the web with automatic refinement by an LLM, we present a framework for efficiently constructing large-scale datasets without manual intervention.
The remainder of this paper is organized as follows. Chapter 2 provides an overview of related research and clarifies the positioning of this study. Chapter 3 details the proposed dataset construction method. Chapter 4 describes the evaluation of the constructed dataset and the performance evaluation experiments of V\&L models using it. Chapter 5 presents the experimental results, and Chapter 6 discusses these results, the limitations of this research, and future prospects. Finally, Chapter 7 concludes the paper.

\section{Related Work}

This chapter provides an overview of existing research related to the construction of Japanese V\&L datasets.

\subsection{Overview of V\&L Datasets}

A V\&L dataset consists of pairs of text and images and is used for training V\&L models. Methods for constructing V\&L datasets can be broadly classified into four categories: (1) those constructed by human annotation, (2) those constructed by web crawling, (3) those constructed by translating datasets from other languages such as English, and (4) those constructed using Large Language Models (LLMs).
These methods and representative datasets constructed by them are detailed below.

\subsection{V\&L Dataset Construction Methods and Existing Datasets}

\subsubsection{Datasets Constructed by Human Annotation}

Datasets constructed by manual annotation generally require significant cost and time. On the other hand, they have the advantage that the image and text content are well-matched, and the text quality is high.
Famous English image caption datasets include Microsoft COCO Captions \cite{lin2014microsoft} and Flickr30k \cite{young2014image}. A representative Japanese dataset is STAIR Captions \cite{Yoshikawa2017}.
STAIR Captions were annotated through crowdsourcing. It was constructed over approximately six months by about 2,100 workers, with five captions provided for each of its 164,062 images.

\subsubsection{Datasets Constructed by Web Crawling}

The web contains a vast number of images and their accompanying texts (such as alt-text), and much research has been conducted on automatically collecting and processing these to construct large-scale V\&L datasets. English datasets constructed with this approach include Conceptual 12M [New Ref. A], and a well-known Japanese dataset is Japanese Image Text Pairs [New Ref. B].
Japanese Image Text Pairs [New Ref. B] constructed approximately 6.6 million Japanese image-caption pairs using images and alt-texts obtained from the web.
While datasets constructed this way are large-scale, due to the nature of alt-text, quality issues may remain, such as the text not being natural Japanese sentences, describing only a small part of the image, or the image and text content not necessarily matching perfectly semantically.

\subsubsection{Datasets Constructed by Translation}

In English-speaking countries, V\&L datasets are more abundant both quantitatively and qualitatively compared to other languages. Therefore, there are attempts to machine-translate these English datasets into other languages (including Japanese). For example, LLaVA-Instruct-150K-JA [New Ref. D] is a dataset created by translating the English instruction-following dataset LLaVA Visual Instruct 150K [New Ref. E] into Japanese using machine translation services like DeepL API.
Such translation-based datasets offer the advantage of relatively easily extending existing large-scale resources to other languages. However, they are prone to issues such as quality degradation due to translation inaccuracies (mistranslations, unnatural expressions) and insufficient reflection of knowledge, context, and cultural phenomena specific to the target language (in this case, Japanese).

\subsubsection{Datasets Constructed Using LLMs}

With the recent improvement in the capabilities of Large Language Models (LLMs), approaches utilizing LLMs for V\&L dataset construction have emerged. LLMs are particularly often used in the construction of "instruction-following data," which is used to train models to generate responses based on specific instructions or commands.
LLaVA Visual Instruct 150K [New Ref. E] is one such instruction-following dataset, constructed using LLMs like GPT-4 [New Ref. F]. The construction procedure for this dataset is as follows:

\begin{enumerate}
  \item Use an existing image caption dataset where multiple captions are assigned to an image (e.g., COCO Captions [4]) as the source data.
  \item Input the image into an object detector (e.g., YOLO) to identify the location information (bounding boxes) of major objects in the image.
  \item Input the existing captions and the detected object location information in text format to GPT-4, and have GPT-4 generate conversational data, detailed image description data, and question-answering data involving complex reasoning. In this process, GPT-4 is fed text information extracted from the image (captions, object information), not the image itself.
\end{enumerate}

Thus, by leveraging the high reasoning and language generation capabilities of LLMs, it becomes possible to generate diverse and complex V\&L datasets. However, there are points to be cautious about with this approach. Firstly, when using the API of a commercial LLM like GPT-4 for dataset construction, it is necessary to comply with its terms of use (e.g., OpenAI's license). For example, OpenAI's terms may prohibit the use of GPT-4 output for training models that compete with OpenAI, which could restrict the commercial use of V\&L models trained on datasets constructed this way. Secondly, it should also be noted that datasets like LLaVA Visual Instruct 150K require an existing source image caption dataset, meaning they are not generating entirely new datasets from scratch but are rather extensions of existing datasets.

\subsection{Positioning of This Research}

Based on the related research described above, the proposed method in this study has the following novelty and advantages.
Existing manually annotated Japanese datasets (e.g., STAIR Captions [3]) are high-quality but small in scale. Datasets automatically constructed by web crawling (e.g., Japanese Image Text Pairs [New Ref. B]) are large-scale but have inconsistent quality, particularly in terms of text naturalness and semantic consistency with images. Translated datasets (e.g., LLaVA-Instruct-150K-JA [New Ref. D]) have issues capturing Japan-specific contexts. Existing dataset construction using LLMs (e.g., LLaVA Visual Instruct 150K [New Ref. E]) can generate high-quality instruction-following data but depends on source data and has licensing issues.

This research aims to strike a balance among these challenges by introducing a process for large-scale image and alt-text pairs collected from the web: (1) assisting image content understanding through object detection, and (2) improving alt-text quality (refining into natural Japanese, enhancing semantic consistency with images) using an LLM. Specifically, it attempts to efficiently construct a large-scale Japanese V\&L dataset of quality comparable to manual annotation, while retaining Japan-specific content, by leveraging the scalability of web collection and automatically improving text quality using LLMs. This is an approach that significantly improves the quality of existing automatically collected datasets and reduces the cultural gap inherent in translated datasets. Thus, this research proposes a new dataset construction paradigm that considers multiple aspects: scalability, quality, and reflection of Japan-specific context.

\section{Method}

The dataset construction procedure is shown in Figure \ref{fig:dataset_construction_procedure}. The construction procedure is roughly divided into the following three stages:

\begin{enumerate}
  \item Acquire images and alt-texts from Common Crawl WARC files.
  \item Perform object detection in images using an object detector.
  \item Refine texts using an LLM.
\end{enumerate}

Alt-text is used as a description of an image, but it may not adequately describe the image or may be in poorly written Japanese. Therefore, by leveraging image information to refine alt-text with an LLM, a higher-quality dataset can be constructed.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{images/pipeline.pdf}
  \caption{Dataset Construction Procedure}
  \label{fig:dataset_construction_procedure}
\end{figure}

\subsection{Acquisition of Images and Alt-texts}

In this study, Common Crawl was used as the source for acquiring images and alt-texts. The repository crawled between August 3, 2024, and August 16, 2024, consisting of 2.3 billion pages, was used.
Since Common Crawl is a dataset crawled from web pages across the entire internet, it includes data from non-Japanese sites and inappropriate data. Therefore, we first filtered the pages. Pages where the lang attribute of the <html> tag was "ja" were considered Japanese sites, and other pages were excluded. Additionally, pages containing inappropriate words were excluded. As a list of inappropriate words, we used a combination of \verb|ja_adult_keywords|, \verb|ja_discrimination_keywords|, and \verb|ja_violence_keywords| from llm-jp-corpus2, and the English list of inappropriate words from List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words3.
From the pages that passed the filtering, images and alt-texts were downloaded. To maintain quality, we excluded: (1) images whose extensions were not jpeg, jpg, or png, (2) alt-texts shorter than 10 characters, and (3) alt-texts that did not contain Japanese.

\subsection{Object Detection in Images}

The model used for tagging images was the Recognize Anything Model (RAM) [20], with the RAM++(14M)*4 checkpoint. Additionally, Grounding DINO [14] was used as a model to obtain the confidence score for each tag.
Finally, tags with high confidence scores were selected and filtered. The confidence threshold was set to 0.25.

\subsection{Alt-text Refinement by LLM}

Finally, the alt-texts were refined using an LLM to create caption data and VQA data. The LLM used was CyberAgentLM3-22B-Chat [9].

\section{Dataset Analysis}
To evaluate our newly constructed datasets, we performed comparative analyses against several existing Japanese V\&L datasets. These included STAIR Captions [3], Japanese Visual Genome VQA (JA-VG-VQA), and a machine-translated version of a subset of GQA (referred to as GQA translation; prompts provided in the Appendix). Our own datasets, termed Caption (Ours) and VQA (Ours), were the primary subjects of this analysis.

\subsection{Statistical Comparison}

A statistical overview of these datasets is presented in Table 1. Our proposed datasets, Caption (Ours) and VQA (Ours), demonstrate a substantially larger scale in terms of both the number of images and texts compared to STAIR Captions, JA-VG-VQA, and GQA translation. Furthermore, our datasets exhibit a significantly larger vocabulary size, indicating greater lexical diversity. The average character count in our caption dataset is higher than that of STAIR Captions and GQA translation, suggesting more descriptive captions. The average character count for questions in our VQA dataset is comparable to that of the GQA translation.

\begin{table*}[t]
  \centering
  \caption{Statistical Comparison of Datasets}
  \label{tab:stat_comparison}
  \begin{tabular}{lrrrr}
    \hline
    Dataset         & \# of images & \# of texts & \# Vocabulary size & Avg. \# of chars \\
    \hline
    STAIR Captions  & 164,062      & 820,310     & 30,195             & 20.96            \\
    JA-VG-VQA       & 99,208       & 793,664     & 22,991             & 9.75             \\
    GQA translation & 148,854      & 3,999,765   & 11,581             & 11.29            \\
    Caption (Ours)  & 3,884,632    & 3,884,632   & 411,537            & 79.62            \\
    VQA (Ours)      & 3,882,892    & 3,882,892   & 400,816            & 54.43            \\
    \hline
  \end{tabular}
\end{table*}

\subsection{Human Evaluation (for VQA Datasets)}

A human evaluation was conducted specifically for the VQA datasets: Japanese Visual Genome VQA (JA-VG-VQA), GQA translation, and our VQA Dataset (Ours). The evaluation focused on five criteria: the perceived "Japaneseness" of the images (i.e., whether the image content is culturally Japanese), the "Japaneseness" of the text, the naturalness of the text, the relevance between questions and answers, and the correctness of the answers. For this evaluation, 100 instances were randomly sampled from each dataset. Each instance was then assessed by three human evaluators on a 1-5 Likert scale for each criterion.

The mean scores and standard deviations from this evaluation are presented for JA-VG-VQA (Table 2), GQA translation (Table 3), and our VQA dataset (Table 4).

To determine the statistical significance of these differences, independent two-sample t-tests (specifically Welch's t-test) were conducted for each item, with a significance level set at $p < 0.05$. The results indicated that our VQA dataset (OURS) was rated significantly higher in terms of the "Japaneseness of Images" (M=2.83) compared to both GQA (M=1.50) and JVG (M=2.50). Similarly, for "Naturalness of Text," OURS (M=4.20) significantly outperformed JVG (M=3.47) and GQA (M=3.87). However, it was observed that OURS (M=3.60) was rated significantly lower on "Correctness of Answer" compared to GQA (M=4.13) and JVG (M=3.87). For "Japaneseness of Text" and "Question-Answer Relevance," while our dataset performed competitively, consistent significant differences across all pairwise comparisons were not observed. A detailed table of p-values from these t-tests will be included in the full paper.

\begin{table*}[t] % ページ上部に配置し、ページ幅全体を使用
  \centering
  \small % フォントサイズを小さくして収まりやすくする
  \caption{Test Results for Mean Differences Between Datasets for Each Evaluation Item. (Sample size $n=300$ per dataset; Independent two-sample t-test, Welch's t-test assumed; Significance level $p < 0.05$)}
  \label{tab:sig_test_results_en_wide_corrected_means}
  % tabular* を使い、幅を \textwidth に指定。@{\extracolsep{\fill}} でカラム間スペースを調整
  \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lllcccl@{}}
    \toprule
    Comparison Pair & Mean ($\bar{X}_1$) & Mean ($\bar{X}_2$) & Mean Diff.      & t-value & p-value    & Significance \\
    & (Dataset 1)      & (Dataset 2)      & ($|\bar{X}_1 - \bar{X}_2|$) &         & (approx.)  & ($p < 0.025$) \\ % Note: Significance criterion updated based on new data
    \midrule
    \multicolumn{7}{@{}l}{\textbf{1. Image Japanese-ness}} \\ \addlinespace % セクションヘッダの@{}lは維持
    OURS vs JVG     & 3.5200 (OURS)     & 2.0133 (JVG)      & 1.5067           & 16.087  & $<0.001$       & Yes          \\
    OURS vs GQA     & 3.5200 (OURS)     & 1.6567 (GQA)      & 1.8633           & 21.393  & $<0.001$       & Yes          \\ \addlinespace
    \multicolumn{7}{@{}l}{\textbf{2. Text Japanese-ness}} \\ \addlinespace
    OURS vs JVG     & 3.4700 (OURS)     & 3.2900 (JVG)      & 0.1800           & 2.064   & $\approx 0.040$ & No           \\ % p=0.0395 > 0.025
    OURS vs GQA     & 3.4700 (OURS)     & 3.2600 (GQA)      & 0.2100           & 2.164   & $\approx 0.031$ & No           \\ \addlinespace % p=0.0308 > 0.025
    \multicolumn{7}{@{}l}{\textbf{3. Text Naturalness}} \\ \addlinespace
    OURS vs JVG     & 3.8167 (OURS)     & 3.6600 (JVG)      & 0.1567           & 1.761   & $\approx 0.079$ & No           \\ % p=0.0788 > 0.025
    OURS vs GQA     & 3.8167 (OURS)     & 3.4733 (GQA)      & 0.3434           & 3.299   & $\approx 0.001$ & Yes          \\ \addlinespace % p=0.0010 < 0.025
    \multicolumn{7}{@{}l}{\textbf{4. Question-Answer Relevance}} \\ \addlinespace
    OURS vs JVG     & 3.9033 (OURS)     & 4.2600 (JVG)      & 0.3567           & 4.133   & $<0.001$       & Yes          \\ % t=-4.1333, p=0.0000
    OURS vs GQA     & 3.9033 (OURS)     & 4.3433 (GQA)      & 0.4400           & 4.946   & $<0.001$       & Yes          \\ \addlinespace% t=-4.9462, p=0.0000
    \multicolumn{7}{@{}l}{\textbf{5. Answer Correctness}} \\ \addlinespace
    OURS vs JVG     & 2.9267 (OURS)     & 4.3367 (JVG)      & 1.4100           & 14.355  & $<0.001$       & Yes          \\ % t=-14.3553, p=0.0000
    OURS vs GQA     & 2.9267 (OURS)     & 3.8767 (GQA)      & 0.9500           & 7.981   & $<0.001$       & Yes          \\ % t=-7.9814, p=0.0000
    \bottomrule
  \end{tabular*}
\end{table*}

\subsection{Example Outputs}

The following are examples of outputs from our constructed datasets. The first example is a caption generated from an image, and the second example is a question-answer pair generated from an image.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{images/cap.pdf}
  \caption{Example Image from Caption Dataset}
  \label{fig:example_caption}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{images/vqa.pdf}
  \caption{Example Image from VQA Dataset}
  \label{fig:example_vqa}
\end{figure*}

\section{Experiment}

\subsection{Experimental Setup}

To assess the utility of our constructed datasets, we conducted experiments using the LLaVA architecture as the learning method.

Figure \ref{fig:vl_train} illustrates the training architecture of the V\&L model. The model consists of a vision encoder and a Large Language Model (LLM) component. The vision encoder processes the image, while the LLM component generates text based on the image features and the input text.
The training process involves two main stages. In the stage 1, the model learns aliginment between images and captions, while in stage 2, it focuses on Visual Question Answering (VQA) tasks. The model is trained using a combination of image-caption pairs and VQA data.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{images/vlm_train.pdf}
  \caption{V\&L Model Training Architecture}
  \label{fig:vl_train}
\end{figure}

The vision encoder employed was siglip-so400m-patch14-384, and the Large Language Model component of the V\&L model was llm-jp/llm-jp-3-7.2b. The training was performed in two stages. In Stage 1, focused on caption pre-training, we compared models trained on a baseline of STAIR Captions against those trained on our Caption (Ours) dataset. In Stage 2, dedicated to VQA fine-tuning, a baseline combination of JA-VG-VQA and GQA Translation was compared against our VQA (Ours) dataset. Models were typically pre-trained with caption data from Stage 1 before being fine-tuned with VQA data from Stage 2.

The performance of V\&L models trained with different combinations of datasets was evaluated on several Japanese benchmarks. These included JA-VG-VQA500 and JA-VLM-Bench-In-the-Wild, which assesses general knowledge about images and reasoning in Japanese; and heron-bench, which specifically focuses on evaluating the understanding of Japan-specific knowledge and Japanese culture. The results are summarized in Table 5.

\subsection{Experimental Results}

The performance of V\&L models trained with different combinations of datasets was evaluated on several Japanese benchmarks. This analysis focuses on JA-VLM-Bench-In-the-Wild, which assesses general vision and language capabilities, and heron-bench, specifically designed to evaluate the understanding of Japan-specific knowledge and Japanese culture. It is important to note the differing evaluation methodologies of these benchmarks: JA-VLM-Bench-In-the-Wild (and other benchmarks like JA-VG-VQA500, though not detailed in this specific table) primarily utilizes ROUGE-L for evaluation, while heron-bench employs LLM-based evaluation. The performance scores are summarized in Table 5.

On the JA-VLM-Bench-In-the-Wild benchmark, models incorporating our Caption (Ours) dataset for Stage 1 pre-training generally showed competitive results. The combination Caption (Ours) + GQA Translation achieved a score of 22.20, with our full pipeline, Caption (Ours) + VQA (Ours), following closely at 22.19. As indicated in Table 1, our proposed caption dataset has a notably longer average character count (54.43 characters) compared to other datasets like STAIR Captions (20.96 characters). Models trained on such data tend to generate longer, more descriptive outputs. The ROUGE-L metric, which measures n-gram overlap, can potentially disadvantage models producing longer outputs if the ground truth texts are more concise, even if the generated text is semantically rich and correct. Thus, while our models perform well, these ROUGE-L scores might not fully capture the semantic quality of the generated longer outputs.

In stark contrast, on heron-bench, which utilizes LLM-based evaluation that prioritizes semantic correctness and coherence, our full pipeline (Caption (Ours) + VQA (Ours)) demonstrated outstanding performance, achieving a score of 52.263. This score significantly surpasses all other dataset combinations. For instance, Stair Captions + VQA (Ours) scored 34.198, and Caption (Ours) + JVG scored 33.725. This substantial margin on an LLM-evaluated, culturally-focused benchmark strongly suggests that models trained on our proposed datasets generate outputs that are not only semantically accurate but also deeply resonant with Japan-specific contexts and cultural nuances. The LLM evaluator, less constrained by exact string matches or length conformity, is better able to recognize this enhanced contextual understanding.

\begin{table*}[t]
  \centering
  \caption{Experimental Results on Japanese V\&L Benchmarks (Scores indicate performance metric, e.g., Accuracy \%). Missing values will be updated upon availability.}
  \label{tab:experimental_results_actual}
  \begin{tabular}{llccc}
    \hline
    Stage 1                 & Stage 2             & JA-VG-VQA500 & \begin{tabular}[c]{@{}c@{}}JA-VLM-Bench-\\In-the-Wild\end{tabular} & Heron-bench     \\
    \hline
    Stair Captions          & JVG                 &              & 17.95                                                              & 31.571          \\
    Stair Captions          & GQA Translation     &              & 20.12                                                              & 20.537          \\
    Stair Captions          & VQA (Ours)          &              & 10.00                                                              & 34.198          \\
    Caption (Ours)          & JVG                 &              & 18.20                                                              & 33.725          \\
    Caption (Ours)          & GQA Translation     &              & \textbf{22.20}                                                     & 19.855          \\
    \textbf{Caption (Ours)} & \textbf{VQA (Ours)} &              & 22.19                                                              & \textbf{52.263} \\
    \hline
  \end{tabular}
\end{table*}

\subsection{Discussion}

The experimental results, when analyzed in conjunction with the evaluation methodologies of the respective benchmarks, provide compelling evidence for the efficacy of our proposed dataset construction strategy. The standout performance on heron-bench—where our full pipeline (Caption (Ours) + VQA (Ours)) achieved a significantly higher score (52.263) than any other combination—is particularly illuminating. Given that heron-bench employs LLM-based evaluation focusing on semantic and contextual appropriateness, this result strongly validates our core hypothesis: our methodology successfully generates datasets that enable V\&L models to produce outputs rich in Japan-specific knowledge and cultural understanding. The LLM evaluator is adept at recognizing this semantic quality, even if the expression style deviates from concise ground truths.

Conversely, on benchmarks like JA-VLM-Bench-In-the-Wild that utilize ROUGE-L, the performance of models trained on our datasets, while competitive, might not fully reflect their underlying capabilities. Our proposed caption dataset encourages the generation of longer, more descriptive text (as shown by its higher average character count in Table 1). ROUGE-L, relying on n-gram overlap with often more succinct reference texts, may penalize such verbose yet semantically accurate outputs. Therefore, the strong (though not top-scoring) performance of our full pipeline (22.19) on JA-VLM-Bench-In-the-Wild can be interpreted as achieving good semantic alignment that is partially masked by the metric's sensitivity to length and structural conformity. The slightly higher score of Caption (Ours) + GQA Translation (22.20) on this benchmark might suggest that GQA-derived VQA data, typically more direct, could lead to outputs that align marginally better with ROUGE-L against certain ground truths when combined with our richer captions.

This understanding of evaluation metric interaction also sheds new light on the human evaluation findings (Section 4.3), where our VQA (Ours) dataset received lower scores for "Correctness of Answer" despite high ratings for text naturalness and image Japaneseness. Human evaluators, much like ROUGE-L with concise references, might have favored brevity or a specific answer format, whereas models trained on our VQA data likely produced more elaborate, contextually rich, but semantically sound responses. The superior heron-bench performance (LLM-evaluated) strongly supports the notion that these responses are indeed "correct" in a broader, more nuanced semantic and cultural sense, a quality that LLM evaluators are better equipped to appreciate.

Thus, the primary contribution of our dataset is the fostering of models that can generate more descriptive, semantically accurate, and culturally nuanced outputs. The perceived performance can vary with the evaluation metric; LLM-based evaluations, as seen with heron-bench, appear more aligned with capturing the depth of understanding that our dataset aims to instill. The results affirm the success of our approach in creating large-scale, high-quality Japanese V\&L datasets that effectively reflect Japan-specific contexts, a crucial step for advancing V\&L research in non-English domains.

The acknowledged limitations regarding potential biases from web data, object detection imperfections, and LLM characteristics remain. Future work should continue to address these, perhaps by refining prompting strategies or exploring ensemble methods for data generation and filtering. Further investigation into evaluation metrics that balance fluency, semantic accuracy, and cultural relevance for tasks involving rich, descriptive language would also be highly beneficial. Expanding the analysis to include JA-VG-VQA500 with these evaluation considerations in mind would provide a more complete picture.

\section{Conclusion}
This paper introduced a novel three-stage methodology for constructing large-scale, high-quality Japanese Vision and Language datasets that reflect Japan-specific knowledge and culture. By combining web crawling, object detection, and Large Language Model-based text refinement, we successfully built a 4 million-pair image caption dataset and a 4 million-instance VQA dataset. Anticipated experimental validation is expected to demonstrate that V\&L models trained on our datasets achieve significantly improved Japanese understanding performance compared to models trained on existing Japanese datasets. This work not only provides valuable resources for the Japanese V\&L community but also presents a scalable and effective paradigm for dataset creation in languages and cultural contexts currently underserved by existing resources. We believe our contributions will facilitate further advancements in Japanese Vision and Language research.

\section*{Limitations}

Since December 2023, a "Limitations" section has been required for all papers submitted to ACL Rolling Review (ARR). This section should be placed at the end of the paper, before the references. The "Limitations" section (along with, optionally, a section for ethical considerations) may be up to one page and will not count toward the final page limit. Note that these files may be used by venues that do not rely on ARR so it is recommended to verify the requirement of a "Limitations" section and other criteria with the venue in question.

\section*{Acknowledgments}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is an appendix.

\end{document}
